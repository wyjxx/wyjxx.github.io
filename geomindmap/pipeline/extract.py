import os
import base64
import requests
import argparse
import json
from openai import OpenAI 
"""
extract.py
----------
This module extracts structured JSON entities (visual, inference, location) 
from raw LLM reasoning text + image, using OpenAI API.
It produces:
- entity.json: flat entity list
- vi_map_info.json: hierarchical visual/inference map
- l_map_info.json: hierarchical location map

Functions:
- extract_entity(image_path, reasoning_path, output_dir): extract entities from reasoning text + image
- vi_map(output_dir, entity, response1_id): build visual/inference map with granularity and parent
- l_map(output_dir, entity, response2_id): build location map with granularity and parent
- extract(image_path, reasoning_path, output_dir): run full extraction pipeline
"""

client = OpenAI(
    api_key = os.getenv("OPENAI_API_KEY"),
)

# check and fix json format using llm
def check_fix_json(str):
    try:
        json.loads(str)
        return str
    except json.JSONDecodeError as e:
        print("JSON is invalid:", e)
        
    prompt = """
        Fix the following JSON string by ensuring it is properly formatted and contains valid JSON syntax. Please output only raw JSON. Do not use any Markdown syntax. Do not modify the original content.
        """
    response = client.responses.create(
        model = "o4-mini", 
        reasoning = { 
            "effort": "medium"
        },
        input = [
            {
                "role": "user",
                "content": [
                    {"type": "input_text", "text":prompt + str},               
                ]
            }
        ]
    )
    fixed_json = response.output_text
    print("Finish Correction: ", fixed_json)

    try:
        json.loads(fixed_json)
        print("Fixed JSON is now valid.")
        return fixed_json
    except json.JSONDecodeError as e:
        raise ValueError(f"LLM returned invalid JSON after fixing: {e}\nGot: {fixed_json}")


### step 1 : extract entity
def extract_entity(image_path, reasoning_path, output_dir):
    # encode image to base64
    with open(image_path, "rb") as image_file:
        base64_image = base64.b64encode(image_file.read()).decode("utf-8")
    # read reasoning text
    with open(reasoning_path, "r", encoding="utf-8") as f:
        reasoning_text = f.read()

    system_prompt = """
    You are a text analysis expert. Please help me process a reasoning trace generated by an LLM during a Geoguessr task.
    Your workflow is: extract key entity terms and categorize them → perform fine-grained classification and association for clue terms → perform fine-grained classification and association for location terms. We will proceed step by step.
    Please return the result in valid JSON format. Do not use any Markdown syntax. The entire response must not contain anything other than JSON.
    """

    extract_entity = '''
    Based on the image content, please extract the following three types of entity keywords from the reasoning text:
    1. l: location — includes all specific locations (continents, countries, administrative regions, cities, towns, villages, streets, and geographic names such as mountains, rivers, lakes, landforms). Do not include vague location terms like city, village, neighbourhood, urban.
    Note that a location can appear as both an adjective and a noun—do not miss any location mentions.
    2. v: visual element — directly observable elements from the image, including man-made objects, natural features, and global/overall scene information.
    3. i: inference and knowledge element — includes information inferred from visual elements, background knowledge, or search-based clues.
    Do not include entities unrelated to human or geographic features.

    Guidelines:
    - Each noun should be a separate entity. For example:
    a tall building with a rectangle window should be split into a tall building and a rectangle window.
    - Keep expressions concise, with entity names preferably no longer than 3 words.
    - Split entities that include regional elements. For example:
    "European road sign" → extract both "Europe" and "European road sign"
    "Munich's road" → extract both "Munich" and "Munich's road"
    "Technical University of Munich" → extract both "Munich" and "Technical University of Munich"
    - Output a complete and non-redundant list of key entities. You may merge semantically similar entities into a single one.

    Output format (JSON only):
    [
    {"entity": "KEY ENTITY", "type": "TYPE"}
    ...
    ]
    Example:
    Input：I see an image featuring a highway with separated lanes, probably in Europe, with tall residential buildings on the left, which indicates typical German Plattenbau. There’s a tall white building displaying a red banner that might say "Augsburger". 
    Output：
    [
    {"entity": "highway", "type": "v"}
    {"entity": "separated lanes", "type": "v"}
    {"entity": "Europe", "type": "l"}
    {"entity": "tall residential buildings", "type": "v"}
    {"entity": "German Plattenbau", "type": "i"}
    {"entity": "Germany", "type": "l"}
    {"entity": "red banner 'Augsburger'", "type": "v"}
    ]

    Important:
    - Strictly follow the example output format
    - Stay faithful to the original text and do not omit 
    - Please output only raw JSON. Do not use any Markdown syntax
    Your task:
    '''

    response1 = client.responses.create(
        model = "o4-mini", 
        reasoning = { 
            "effort": "medium"
        },
        input = [
            {
                "role": "system",
                "content": [
                    {"type": "input_text", "text": system_prompt},
                ]
            },
            {
                "role": "user",
                "content": [
                    {"type": "input_text", "text": extract_entity + reasoning_text},     
                    {"type": "input_image", "image_url": f"data:image/png;base64,{base64_image}"}        
                ]
            }
        ]
    )
    entity = response1.output_text
    print(entity)

    # save entity to output
    entity = check_fix_json(entity)

    with open(output_dir + "entity.json", "w", encoding="utf-8") as f:
        f.write(entity + "\n")
    print(f"Finish extract_entity! Written in {output_dir}")
    
    return entity, response1.id, response1.usage.total_tokens

def entity_to_vi_l(entity, type):
    # string to json
    entity_json = json.loads(entity)

    # json split to entity_vi and entity_l and return as string
    if type == "vi":
        entity_vi = [e for e in entity_json if e['type'] == 'v' or e['type'] == 'i']
        return json.dumps(entity_vi, ensure_ascii=False, indent=2)
    elif type == "l":
        entity_l = [e["entity"] for e in entity_json if e['type'] == 'l']
        return json.dumps(entity_l, ensure_ascii=False, indent=2)
    else :
        raise ValueError("type must be 'vi' or 'l'")


### step 2 : assign granularity and parent node to v and i entities
def vi_map(output_dir, entity, response1_id):    
    
    entity_vi_text = entity_to_vi_l(entity, "vi")
    
    assign_granularity_parent_vi = ''' 
    Please categorize the clue words into three granularity levels as follows and assign a parent node:
    
    Granularity: 1 -> Entities of type v, representing broad-scope entities or entity groups (e.g., buildings, vegetation, roads, signs, sky)
    Parent node guideline: Assign parent to null.
    
    Granularity: 2 -> Entities of type v, representing entity details (e.g., windows on a building, shrubs in vegetation, markings on a road)
    Parent node guideline: Select a node from granularity 1 as the parent node to indicate its closest association, if no suitable parent is found, leave the parent as null.
    
    Granularity: 3 -> All entities of type i. AND All left entities of type v, representing characteristic of entity details (e.g., glass color of the windows on a building)
    Parent node guideline: Select a node from granularity 2 or 1 as the parent node to indicate its closest association, if no suitable parent is found, leave the parent as null.

    Rules for assigning parent:
    - Each entity only have one parent node
    - Nodes with granularity = 1 must have "parent": null
    - The parent must be a node from a higher level, i.e., granularity-1, granularity-2, etc.
    - First search through all nodes in granularity-1. If no suitable parent is found, continue to granularity-2, and so on until granularity=1
    
    Output format (JSON only):
    [
    {"entity": "ENTITY", "type": "TYPE"，"granularity"：LEVEL_NUMBER，“parent”：“PARENT ENTITY”}
    ...
    ]
    Example:
    Input:
    [
    {"entity": "highway", "type": "v"}
    {"entity": "separated lanes", "type": "v"}
    {"entity": "lanes color", "type": "v"}
    {"entity": "Europe", "type": "l"}
    {"entity": "tall residential buildings", "type": "v"}
    {"entity": "German Plattenbau", "type": "i"}
    {"entity": "Germany", "type": "l"}
    {"entity": "red banner 'Augsburger'", "type": "v"}
    ]
    Output:
    [
    {"entity": "highway", "type": "v", "granularity":1,"parent":null}
    {"entity": "separated lanes", "type": "v", "granularity":2,"parent":"highway"}
    {"entity": "lanes color", "type": "v", "granularity":3,"parent":"separated lanes"}
    {"entity": "tall residential buildings", "type": "v", "granularity":1,"parent":null}
    {"entity": "German Plattenbau", "type": "i", "granularity":3,"parent":"tall residential building"}
    {"entity": "red banner 'Augsburger'", "type": "v", "granularity":2,"parent":"tall residential building"}
    ]

    Important:
    - Strictly follow the example output format
    - Do not assign a parent with the same granularity level! Forbidden!
    - Stay faithful to the original text and do not omit anything
    - Please output only raw JSON. Do not use any Markdown syntax
    Your task:
    '''
    response2 = client.responses.create(
        model = "o4-mini",
        reasoning = { 
            "effort": "medium"
        }, 
        previous_response_id = response1_id,
        input = [
            {
                "role": "user",
                "content": [
                    {"type": "input_text", "text":assign_granularity_parent_vi + entity_vi_text},               
                ]
            }
        ]
    )

    vi_map_info = response2.output_text
    print(vi_map_info)

    vi_map_info = check_fix_json(vi_map_info)
    # save vi_map_info to output
    with open(output_dir + "vi_map_info.json", "w", encoding="utf-8") as f:
        f.write(vi_map_info + "\n")
    print(f"Finish vi_map! Written in {output_dir}")

    return response2.id, response2.usage.total_tokens


### step 3 : assign granularity and parent to location entities
def l_map(output_dir, entity, response2_id):
    
    entity_l_text = entity_to_vi_l(entity, "l")

    assign_granularity_parent_l = ''' 
    Your task is to first classify all location entities into five hierarchical levels based on their geographic scope and output them in order.
    Granularity Level Definitions:
    - 1 = continent (e.g., continents, oceans; ~2500 km scale)
    - 2 = country (e.g., countries, transnational geographic features like mountain ranges; ~750 km scale)
    - 3 = area (e.g., administrative regions within a country: states, provinces, regional landforms; ~250 km scale)
    - 4 = city (e.g., cities, towns, villages, small water bodies, peaks; ~25 km scale)
    - 5 = street (e.g., urban districts, neighborhoods, landmarks, streets; ~1–10 km scale)
    Note: For ambiguous geographic terms (e.g., water bodies, mountain ranges, plains), use the approximate km range above to determine the level.
    Then, for each word, you must assign an appropriate parent node to indicate its closest association:
    - Nodes with granularity = 1 must have "parent": null
    - The parent must be a node from a higher level, i.e., granularity-1, granularity-2, etc.
    - First search through all nodes in granularity-1. If no suitable parent is found, continue to granularity-2, and so on until granularity=1
    - If no suitable parent is found after reaching granularity=1: Leave the parent as null
    - Parents must not be from the same or lower level (forbidden: granularity, granularity+1, ...)
    Exmaple:
    Input:
    [
    {"entity": "Marienplatz"},
    {"entity": "Alps"},
    {"entity": "Europe"},
    {"entity": "Germany"}，
    {"entity": "Tokyo"}，
    {"entity": "Innsbruck"}，
    {"entity": "Southern Germany"},
    {"entity": "Bavaria"},
    {"entity": "Munich"},
    {"entity": "Altstadt"},
    ]
    Output：
    [
    {"entity": "Europe", "granularity": 1, "parent": null},
    {"entity": "Alps", "granularity": 2, "parent": "Europe"},
    {"entity": "Germany", "granularity": 2, "parent": "Europe"},
    {"entity": "Southern Germany", "granularity": 3, "parent": "Germany"},
    {"entity": "Bavaria", "granularity": 3, "parent": "Germany"},
    {"entity": "Tokyo", "granularity": 4, "parent": null}，
    {"entity": "Innsbruck", "granularity": 4, "parent": "Alps"},
    {"entity": "Munich", "granularity": 4, "parent": "Bavaria"},
    {"entity": "Altstadt", "granularity": 5, "parent": "Munich"},
    {"entity": "Marienplatz", "granularity": 5, "parent": "Munich"}
    ]
    Important:
    - Strictly follow the example output format
    - Stay faithful to the original text and do not omit anything
    - Think carefully before assigning parents,follow the rules exactly!
    - Please output only raw JSON. Do not use any Markdown syntax
    Your task:
    '''
    response3 = client.responses.create(
        model = "o4-mini",
        reasoning = { 
            "effort": "medium"
        }, 
        previous_response_id = response2_id,
        input = [
            {
                "role": "user",
                "content": [
                    {"type": "input_text", "text":assign_granularity_parent_l + entity_l_text},               
                ]
            }
        ]
    )

    l_map_info = response3.output_text
    print(l_map_info)

    l_map_info = check_fix_json(l_map_info)
    # save l_map_info to output
    with open(output_dir + "l_map_info.json", "w", encoding="utf-8") as f:
        f.write(l_map_info + "\n")
    print(f"Finish l_map! Written in {output_dir}")

    return response3.usage.total_tokens


def extract(image_path, reasoning_path, output_dir):
    """
    Run full extraction pipeline to generate GeoMindMap layout info:
    1. entity.json
    2. vi_map_info.json
    3. l_map_info.json
    Returns total tokens used.
    """
    # step 1 : extract entity
    entity, response1_id, token1= extract_entity(image_path, reasoning_path, output_dir)

    # step 2 : assign granularity and parent node to v and i entities
    response2_id, token2= vi_map(output_dir, entity, response1_id)

    # step 3 : assign granularity and parent to location entities
    token3 = l_map(output_dir, entity, response2_id)

    print("Extract completed successfully!")
    return token1 + token2 + token3

